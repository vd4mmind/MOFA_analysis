\documentclass[10pt, a4paper,openany]{report}

%% Load packages %%
%\usepackage[applemac]{inputenx}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}     % Mathematical writing
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{hyperref}
\usepackage{xcolor} 
\usepackage{enumitem}    % Control layout of itemize, enumerate, description
\usepackage[font={small}, margin=20pt]{caption}     % Captions for figures
\usepackage[font={footnotesize}, margin=20pt]{subcaption}  % Captions for subfigures
\usepackage[nameinlink]{cleveref}    % Enhances corss-referencing features, allowing the format of referencs to be determined automatically according to the type of reference
\usepackage[paper=a4paper,margin=1in]{geometry} % Margins

\usepackage[hyperref=true, url=false, backref=false, backend=biber, style=authoryear, citestyle=numeric-comp]{biblatex}
%\usepackage[hyperref=true, url=false, backref=false, backend=biber, style=numeric, citestyle=numeric-comp, doi=false, isbn=false]{biblatex}

%\renewcommand{\thesection}{\Roman{section}}
%\renewcommand{\thesubsection}{\thesection.\Roman{subsection}}
\renewcommand{\thesection}{\arabic{section}}

\setlength{\parindent}{0pt}

%\bibliographystyle{abbrv}
\bibliography{references}
%\addbibresource{references}

%% Load personalised commands %%
\input{utils.tex}

% Reference names
%\crefname{equation}{equation}{equations}
%\crefname{figure}{figure}{figures}
%\crefname{section}{section}{sections}

\title{Methods for: Multi-omics factor analysis disentangles patient heterogeneity in chronic lymphocytic leukaemia}
\author{Ricard Argelaguet, Britta Velten, Damien Arnol, …, Sascha Dietrich, Thorsten Zenz, \\ John C. Marioni, Florian Buettner, Wolfgang Huber, Oliver Stegle}
%\date{ }



\begin{document}
	
\maketitle
\tableofcontents

\newgeometry{top=1in,bottom=1in,right=1in,left=1in}


\section{Multi-Omics Factor Analysis model}

\subsection{Model description}
The Multi-Omics Factor Analysis (MOFA) model builds upon the Group Factor Analysis framework \cite{virtanen2012bayesian,klami2015group,bunte2016sparse,zhao2016bayesian}, which aims at explaining dependencies between groups of variables (views) instead of dependencies between individual variables, as standard factor analysis does \cite{basilevsky2009statistical}. In particular, we extended the model proposed in \cite{virtanen2012bayesian} with three key features to make it applicable to a wide range of multi-omics data sets: (a) Two level of sparsities combined with fast variational inference, (b) explicit modelling of non-gaussian data types, and (c) handling of missing values.\\
This section describes the basic mathematical details of the Gaussian model.\\

The input data consists of $M$ matrices $\bfY^m \in \R^{N \times D_m}$ which are decomposed as follows:
\[
\mathbf{Y}^m = \mathbf{Z}\mathbf{W}^{mT} + \bepsilon^m
\]
where  $\bfZ \in \R^{N \times K}$ are the low-dimensional latent variables, $\bfW^m \in \R^{D_m \times K}$ are the loadings that relate the high-dimensional space and low dimensional representations, and $\bepsilon^m \in \R^{N \times D_m}$ is the noise term. 
Conventionally, we define the noise to be normally distributed with zero mean and diagonal covariance matrix, which corresponds to heteroskedastic observations:
\[
p(\epsilon^m_d) = \Ndist{\epsilon^m_d}{0,1/\tau_d^m},
%\Psi = \Ndist{\Psi}{0,\Tau}
\]
resulting in the following normal likelihood:
\[
%y_{\cdot d}^m \sim N(ZW_{d,\cdot}^{mT},\tau_d^m \mathds{1}_n).
p(y_{nd}^m) = \Ndist{y_{nd}^m}{\bfw_{d,:}^m\bfz_{n,:},1/\tau_d^m}
\]
We fit this model in a Bayesian framework and place prior distributions on the weights $\bfW^m$, the latent variables $\mathbf{Z}$ as well as on the precision of the noise $\bTau^m$.
Conventionally, we use a standard Gaussian prior on the latent variables:
\[
p(z_{n,k}) = \Ndist{z_{n,k}}{0,1}
\]
and a Gamma distribution for the precision of the noise:
\[
p(\tau_d^m) = \Gdist{\tau_d^m}{a_0^\tau,b_0^\tau}
\]
The prior distribution on the weight matrices is the most important part of the model as it encodes the sparsity assumptions. Here we incorporate two levels of sparsity: a view- and factor-wise sparsity and a feature-wise sparsity. The aim of the factor and view-wise sparsity is to identify which factors are active in which view, such that the weight vector $\bfw_{:,k}^m$ is shrunk to zero if the factor $k$ is not driving any variation in view $m$. 
In contrast, the feature-wise sparsity enforces zero-weights in individual features that do not drive variation, which yields an interpretable solution with a small number of active features.\\

To encode both sparsity levels we use a combination of an Automatic Relevance Determination (ARD) prior for the view- and factor-wise sparsity and a spike-and-slab prior for the feature-wise sparsity, similar to \cite{khan2014identification}. This can be written as:   
\begin{align}
\label{eq:wprior}
p(\hat{w}_{d,k}^m,s_{d,k}^m) &= \mathcal{N} (\hat{w}_{d,k}^m | 0, \frac{1}{\alpha_k^m}) \text{Ber}(s_{d,k}^m|\theta_k^m)\\
p(\theta_k^m) &= \Bdist{\theta_k^m}{a_0^\theta,b_0^\theta}\\
p(\alpha_k^m) &= \Gdist{\alpha_k^m}{a_0^\alpha, b_0^\alpha}
\end{align}
In this formulation $\alpha_k^m$ controls the strength of factor $k$ in view $m$. Therefore, in practice, the ARD prior yields a matrix $\balpha \in \R^{M \times K}$ that defines three different types of factors:
\begin{itemize}
	\item Factors that explain variation in a single data set (unique factors): all elements in the column vector $\balpha_{:,k}$ are very large except one.
	\item Factors that explain variation in a subset of data sets (partially shared factors): some elements in the column vector $\balpha_{:,k}$ are very large whereas others are small.
	\item Factors that explain variation in all data sets (fully shared factors): all elements in the column vector $\balpha_{:,k}$ are small.
\end{itemize}
In contrast, the feature-wise sparsity models each individual weights as a mixture of a point distribution at zero (spike) and a normal distribution (slab), thereby allowing weights of non-important features to have high density at zero. However, the presence of a Dirac delta mass function makes the application of variational inference troublesome, so here we used a re-parameterization of the spike and slab prior as a product of a Gaussian random variable and a Bernoulli random variable that is more amenable to variational inference \cite{titsias2011spike}.  The degree of contribution from the spike term, or the probability of success in the Bernoulli distribution in the re-parametrised form,  is controled by $\theta_k^m$, which is also learnt by the model.\\

This completes the definition of the model, which is graphically illustrated in Fig S1. The joint probability density function is:
\begin{align}
\begin{split}
p(\bfY,\hat{\bfW},\bfS,\bfZ,\bTheta)  = &\prod_{m=1}^{M} \prod_{n=1}^{N} \prod_{d=1}^{D_m} \Ndist{y_{nd}^m}{\sum_{k=1}^{K} s_{dk}^m \hat{w}_{dk}^m z_{nk},1/\tau_d} \\
& \prod_{d=1}^{D_m} \prod_{k=1}^{K} \Ndist{\hat{w}_{dk}^m}{0,1/\alpha_k^m} \text{Ber}(s_{d,k}^m|\theta_k^m) \\
& \prod_{n=1}^{N} \prod_{k=1}^{K} \Ndist{z_{nk}}{0,1} \\
& \prod_{m=1}^{M} \prod_{k=1}^{K} \Bdist{\theta_k^m}{a_0^\theta,b_0^\theta}
\label{likelihood}
\end{split}
\end{align}

\subsection{Model inference}
To ensure scalable inference we use a variational approach with a mean-field approximation for all variables except the spike-and-slab weights \cite{bishop2006pattern}, where we adapt prior work from \cite{titsias2011spike} and introduce a paired mean field approximation $q(w_{d,k}^m,s_{d,k}^m)$. The core idea of variational Bayes is to approximate the true posterior distribution over all unobserved variables using a variational distribution that has a factorized form, which allows the derivation of a simple iterative inference scheme. The approximated variational distribution is then optimised to get as close as possible to the true distribution by minimising the Kullback-Leibler divergence, or equivaently, a lower bound on the marginal likelihood, also called evidence lower bound (ELBO) \cite{bishop2006pattern}. The assumed form of the variational distribution is the following:
\begin{align*}
&q(\bfZ,\bfS,\bfW,\balpha,\bTau,\btheta) = q(\bfZ)q(\balpha)q(\btheta)q(\btau)q(\bfS\bfW) =  \\
&\prod_{n=1}^{N} \prod_{k=1}^{K} q(z_{n,k}) \prod_{m=1}^{M} \prod_{k=1}^{K} q(\alpha_k^m) q(\theta_k^m)
\prod_{m=1}^{M} \prod_{d=1}^{D_m} q(\tau_d^m) \prod_{k=1}^{K} q(w_{d,k}^m,s_{d,k}^m) 
\end{align*}
This factorised approximation ensures that the model scales linearly with the number of factors, the number of features, the number of views and number of samples (Fig SX).\\
For details on the inference and the update equations see the \nameref{appendix}.

\subsection{Integration of non-gaussian views}
To implement efficient variational inference in conjunction with a non-Gaussian likelihood we adapt prior work from \cite{seeger} using local variational bounds. The key idea is to approximate non-Gaussian data by a normally-distributed pseudo-data based on a second-order Taylor expansion. This allows to efficiently re-use the variational updates from the Gaussian case. The approximation requires the introduction of variational parameters that are adjusted alongside the updates to iteratively improve the fit. The exact form of the Gaussian pseudo-data distribution depends on the likelihood and the lower bound of gaussian form chosen on it.\\
Here we implemented two examples of non-Gaussian likelihoods, a Bernoulli likelihood to model binary data and a Poisson likelihood to model count data.\\
For details on the derivation and the update equations see the \nameref{appendix}.


%\begin{align*}
%	\tilde{y}_{nd} &= \frac{2y_{nd}-1}{4 \mathrm{S}(\zeta_{nd})}, \qquad\text{and}\\
%	\hat{y}_{nd} &= \zeta_{nd} - \frac{\lambda'(\zeta_{nd})(1-\frac{y_{nd}}{\lambda(\zeta_{nd})})}{\kappa_d},
%\end{align*}
%respectively. Here,  $\lambda(\zeta) = \log(1+e^\zeta)$, $\mathrm{S}(\zeta)=\frac{1}{4\zeta}\tanh\left(\frac{\zeta}{2}\right)$ and $\zeta_{nd}$ denotes the variational parameter updated alongside with the model parameters.


\subsection{Handling of missing values}
The model naturally accounts for missing values, as non-observed data points do not intervene in the likelihood and can be ignored in the update equations. In practice, we use a binary mask $\mathcal{O}^m \in \mathbb{R}^{N\times D_m}$ for each view $m$, such that $\mathcal{O}_{n,d} = 1$ when feature $d$ is observed for sample $n$, 0 otherwise.

\subsection{Learning the number of factors}
The model automatically learns the dimensionality of the latent factor space by setting factors to zero, during training, if they do not explain significant variation in any view. This is achieved by the view and factor wise ARD prior (equation \ref{eq:wprior}). In practice, a threshold is required to define a factor as active or inactive. Here we define a factor as inactive, and therefore dropped from the model during training, if it explained less than 3\% of variation in all views.\\

\subsection{Convergence}
In contrast to sampling methods, variational approximations have the appealing property that convergence is easily monitored by changes in the ELBO, which is required to increase monotonically \cite{bishop2006pattern}. In practice, we set the default threshold for convergence as a change in ELBO smaller than 0.1.\\

\subsection{Centering and scaling of the data}
MOFA does not require the data to be centered or scaled. The first property is achieved by incorporating a constant vector of ones in the latent factor matrix, and initialising the corresponding weight vector to the true means. This ensures that the rest of the factors capture variation independent of the mean.\\
The second property is achived by the factor and view sparsity, which ensures different scale of the weights for each view.

\subsection{Model training and robustness}
A drawback of the iterative variational Bayes algorithm is that it is not guaranteed to find the optimal solution \cite{bishop2006pattern}. Consequently, we follow common practice \cite{hore2016tensor} and ran the method multiple times under different initialisations, and we  subsequently  select the model with the highest ELBO. Additionally, it is recommended to check that the factors are consistently found across multiple runs. 

\subsection{Downstream analysis functions: pipeline for factor annotation}
To do...

\subsection{Implementation}
What do we describe here?


\section{Model validation using simulations}

\subsection{Recovery of the true number of factors}
To assess the technical capabilities of MOFA we validated the model using observations simulated from the generative model, where we varied the number of views, the number of features, the number of factors and the fraction of missing values. In particular, to constrain our simulation to realistic multi-omics scenarios, we ranged the number of views from 1 to 20, the number of features from 100 to 10,000, the dimensionality of the latent space from 5 to 60 and the percentage of missing values from 10 to 90\%.\\
All trials were started with a sufficiently high number of factors ($k=100$). To test the robustness under different initialisations, ten models were trained for every simulation scenario.\\

In most of the settings the model robustly recovers the correct number of factors. Exceptions occur when the dimensionality of the latent space is too large (more than 50 factors) (Fig S2a) or when an excessive amount of missing values (more than 80\%) is present in the data (Fig S2d). Little variability is observed across different initialisations.

%\subsection{Recovery of the true number of factors}
%A key aspect of MOFA is the use of a spike-and-slab prior distribution to enforce feature-wise sparsity on the loadings, which yields a more interpretable solution. To assess the effect of the spike-and-slab prior we simulated data using the generative model and we fit two sets of models: a group of models with spike-and-slab sparsity and a set of models with feature-wise Automatic Relevance Determination sparsity, which is a softer sparsity commonly used in Bayesian factor analysis models. Also, we compared both solutions to a traditional Principal Component Analysis fit on the concatenated data set. As shown in Fig S3a, the weights of the spike and slab models are more zero-inflated and have higher density on the tails than the non-sparse models, suggesting that only a small subset of features are driving each factor.

\subsection{Non-Gaussian likelihoods}
A key improvement of MOFA with respect to previous methods is the use of non-Gaussian likelihoods to properly model different data modalities. In particular, we implemented a Bernoulli likelihood to model binary data and a Poisson likelihood to model count data.\\
To assess the performance of both likelihood models, we simulated binary and count data using the generative model and we fit two sets of models for each data type: a group of models with a Gaussian likelihood and a group of models with a Bernoulli or Poisson likelihood, respectively.\\
Although both likelihoods are able to recover the true number of factors, the models with the non-Gaussian likelihoods clearly result in a better fit to the data, with distributions of the predicted values closer to the true shape of the non-gaussian distributions (Fig S4 and Fig S5). 

\subsection{Disentangling sources of variation}
The main task of MOFA is to disentangle the different sources of variation in a multi-view data set. To evaluate its performance on this task, we simulated data from the generative model were the factors were clearly set to be active or inactive in a specific view, and we assessed whether  MOFA model recover the true activity of the factors. Subsequently, we also compared the performance with iCluster, a commonly used method in multi-omics studies. In principle, the latent variable model underlying iCluster is focused on clustering of samples, but it can also be used to perform variance decomposition. However, its underlying assumptions are not suited for this task.\\ 
MOFA correctly infers the true activity pattern of the factors per view in all settings while iCluster infers incorrect sharedness of factors across views, especially with increasing dimensionality of the latent space (Fig. S7).



\section{Detailed methods on CLL analysis}

\subsection{Data processing}
{\color{red} The data was obtained from \cite{cllpaper}. Details on the data generation and processing can be found there.}
For the training of MOFA we included 62 drug response measurements (excluding NSC 74859 and bortezomib due to bad quality) at five concentrations each ($p=310$) with a threshold at 1.1 to remove outliers.  Mutations were considered if present in at least 3 samples ($p=69$). Low counts from RNAseq data were filtered out and the data was normalized using the \textit{estimateSizeFactors} and \textit{varianceStabilizingTransformation} function of the DESeq2 \cite{love2014moderated}. For training we used the top $p=5000$ most variable mRNAs after exclusion of genes from the Y chromosome. Methylation data was transformed to M-values and the top $1\%$ CpG sites excluding sex chromosomes ($p=4248$) were included.
We included patients diagnosed with CLL and having data in at least two views into the MOFA model leading to $n=200$ samples.

\subsection{Robustness}
Applying MOFA to this data set we recovered up to ten latent factors explaining a minimum of 3\% of variation in at least one view (Figure S8a,b). The inferred factors and weights are robust to the random initializations across 25 runs (Figure S8c,d). Also, the MOFA factors show near orthogonality, as opposed to the strongly correlated factors inferred by iCluster (Figure S9).\\
A single model was selected for down-stream analysis  based on the highest evidence lower bound, which is highlighted in FigX.

\subsection{Inspection of loadings}
The first step on characterisation of factors is the direct inspection of loadings. Importantly, the scale of the weights infered by the MOFA model are proportional to the scale of the corresponding observations. Therefore, the weights of views with different scale (i.e mRNA and drug response) cannot be directly compared. Only the values of weights from the same view can be directly compared. For this reason, for visualisation purposes and to facilitate interpretation all loadings are always displayed in a relative scale from 0 to 1. \\

\subsection{Gene set enrichment analysis}
Gene Set Enrichment Analysis is performed using a parametric t-test comparing the means of the foreground set (the weights of genes that belong to gene set $i$) and the background set (the weights of genes that do not belong to gene set $i$). p-values are adjusted for multiple testing using FDR correction.

\subsection{Downsampling analysis}
To finish...

\subsection{Imputation}
Unobserved data points are directly imputed from the MOFA model equation
\[
\mathbf{Y}^m = \mathbf{Z}\mathbf{W}^{mT}
\]
In case of missing data points for samples on latent factors those are points are dropped from the product.
For non-gaussian views imputations are based on the corresponding link functions of the generalised linear model, i.e.  
\[
\mathbf{Y}^m = \frac{\exp(\mathbf{Z}\mathbf{W}^{mT})}{1+\exp(\mathbf{Z}\mathbf{W}^{mT})}
\]
\[
\mathbf{Y}^m = \exp(\mathbf{Z}\mathbf{W}^{mT})
\]
for Bernoulli and Poisson views, respectively, and are rounded to integer if imputations in the range of the data are wanted.

To compare imputation performance we trained MOFA on the data of samples available in all measurement ($n=121$) and masked either single values or a full sample in the drug response view at random. The number of factors used for the task was learned by MOFA, where depending on the amount of full missing cases the focus on inferring shared factors was increased. For values missing at random factors were dropped when explaining less than 0.001 of variance in all views, for full cases missing we used 0.1 as a threshold.

\subsection{Survival Analysis}
In order to assess the association of MOFA factors with clinical outcome we used time to next treatment as response variable in a Cox proportional hazard model including all patients, for which this information was available $n=174$, 96 uncensored cases). For univariate associations (as shown in Figure 5b) we scaled all predictors to ensure comparability of the hazard ratios and oriented factors such that their Hazard ratio is greater or equal to 1 due to rotational invariance of the factors. 

To investigate the predictive power of different datasets, we used a multivariate Cox model and compared Harrell's C-index of predictions in a stratified 5-fold cross-validation scheme. As predictors we included the top 10 principal components on the data of each single view as well as a concetenated data set ('all') as well as the 10 MOFA factors. Missing values in a view were imputed by the feature-wise mean. In a second set of models we used the complete set of all features in a view and used a ridge penalty in the Cox model as implemented in the R package \textit{glmnet} to get predictions based on each view as well as the concatenated data, which leads to similar prediction performance as the principal component approach.

The Kaplan Meier plots were generated using an optimal cut point on each factor calculated based on the maximally selected rank statistics as implemented in the R package \textit{survminer}.

\newpage

\section{Appendix} \label{appendix}

\subsection{Update equations of the gaussian model}

The optimal distribution $\hat{q}_i$ for each variable $\bfx_i$, is the following:
\begin{equation} \label{eq:optimal_q_dist}
\log \hat{q}_i(\bfx_i) = \E_{i \neq j} [\log p(\bfY,\bfX)] + \const
\end{equation}
where $\E_{i \neq j}$ denotes an expectation with respect to the $q$ distributions over all variables $\bfx_j$ except for $\bfx_i$.
The additive constant is set by normalising the distribution $\hat{q}_i(\bfz_i)$:
\[
\hat{q}_i(\bfx_i) = \frac{\exp(\E_{i \neq j}[\log p(\bfY,\bfX)])}{\int \exp(\E_{i \neq j}[\log p(\bfY,\bfX)]) d\bfX}
\]

\subsubsection*{Latent variables}
Term from the likelihood $p(\bfY|\hat{\bfW},\bfZ,\btau,\bfS)$:
\begin{align*}
&\sum_{m=1}^{M} \sum_{d=1}^{D=m} \la\tau_d^m\ra \la s_{dk}^m \hat{w}_{dk}^m \ra y_{nd}^m z_{nk}
-\frac{1}{2}\sum_{m=1}^{M} \sum_{d=1}^{D_m} \la\tau_d^m\ra \la (s_{dk}^m\hat{w}_{dk}^m)^2 \ra z_{nk}^2 \\
&-\frac{1}{2}\sum_{m=1}^{M} \frac{1}{2}\sum_{d=1}^{D} \la\tau_d^m\ra \sum_{j \neq k} (\la s_{dj}^m \hat{w}^m_{dj} \ra \la z_{nj} \ra) \la\hat{w}^m_{dk} s_{dk}^m \ra \la z_{nk} \ra + \const
\end{align*}
Term from the prior $p(z_{nk})$:
\[
-\frac{1}{2} z_{nk}^2 + \const
\]
Variational distribution:
\[
q(\bfZ) = \prod_{k=1}^{K} \prod_{n=1}^{N} q(z_{nk}) = \prod_{k=1}^{K} \prod_{n=1}^{N} \Ndist{z_{nk}}{\mu_{z_{nk}},\sigma_{z_{nk}}}
\]
where
\[
\sigma_{z_{nk}}^2 = \Big( \sum_{m=1}^{M} \sum_{d=1}^{D_m} \tau_d^m \la (s_{dk}^m\hat{w}_{dk}^m)^2 \ra + 1 \Big)^{-1}
\]
\[
\mu_{z_{nk}} = \sigma_{z_{nk}}^2 \sum_{m=1}^{M} \sum_{d=1}^{D_m} \la\tau_d^m\ra \la s_{dk}^m\hat{w}_{dk}^m \ra \Big( y_{nd}^m - \sum_{j \neq k} \la s_{dj}^m \hat{w}_{dj}^m \ra \la z_{nj} \ra \Big)
\]

\subsubsection*{Spike and Slab Weights}
Variational distribution:
\[
q(\hat{\bfW},\bfS) = \prod_{m=1}^{M} \prod_{d=1}^{D_m} \prod_{k=1}^{K} q(\hat{w}_{dk}^m,s_{dk}^m) = \prod_{m=1}^{M} \prod_{d=1}^{D_m} \prod_{k=1}^{K} q(\hat{w}_{dk}^m|s_{dk}^m)q(s_{dk}^m)
\]
Update for $q(s_{dk}^m)$:
\[
\gamma_{dk} = q(s_{dk}=1) = \frac{1}{1+\exp(-\lambda_{dk})}
\]
where
\begin{align*}
\lambda_{dk}^m = \la \log\frac{\theta}{1-\theta} \ra + 0.5\log\frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} - 0.5\log\Big( \sum_{n=1}^{N} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} \Big) \\
+ \frac{\la\tau_d^m\ra}{2} \frac{ \Big( \sum_{n=1}^{N} y_{nd}^m \la z_{nk} \ra - \sum_{j \neq k} \la s_{dj}^m\hat{w}_{dj}^m\ra \sum_{n=1}^{N} \la z_{nk} \ra \la z_{nj} \ra \Big)^2} {\sum_{n=1}^{N} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} }
\end{align*}
Update for $q(\hat{w}_{dk}^m)$:
\begin{align*}
q(\hat{w}_{dk}^m|s_{dk}^m=0) = \Ndist{\hat{w}_{dk}^m}{0,1/\alpha_k^m} \\
q(\hat{w}_{dk}^m|s_{dk}^m=1) = \Ndist{\hat{w}_{dk}^m}{\mu_{w_{dk}^m},\sigma_{w_{dk}^m}^2}
\end{align*}
where
\[
\mu_{w_{dk}^m} = \frac{ \sum_{n=1}^{N} y_{nd}^m \la z_{nk} \ra - \sum_{j \neq k} \la s_{dj}^m\hat{w}_{dj}^m \ra \sum_{n=1}^{N} \la z_{nk} \ra \la z_{nj} \ra } { \sum_{n=1}^{N} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} }
\]
\[
\sigma_{w_{dk}^m} = \frac{ \la\tau_d^m\ra^{-1} } { \sum_{n=1}^{N} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} }
\]
Taken together this means that we can update $q(\hat{w}_{dk}^m,s_{dk}^m)$ using:
\[
q(\hat{w}_{dk}^m|s_{dk}^m) \times q(s_{dk}^m) = \Ndist{ \hat{w}_{dk}^m } { s_{dk}^m \mu_{w_{dk}^m}, s_{dk}^m\sigma_{w_{dk}^m}^2 + (1-s_{dk}^m)/\alpha_k^m} \quad \times \quad (\lambda_{dk}^m)^{s_{dk}^m} (1-\lambda_{dk}^m)^{1-s_{dk}}
\]


\subsubsection*{ARD precision (alpha)}
\begin{equation} \label{eq:optimal_q_dist}
\log \hat{q}_i(\bfx_i) = \E_{i \neq j} [\log p(\bfY,\bfX)] + \const
\end{equation}
Term from the prior $\log p(\alpha_k^m)$:
\[
(a_0^\alpha-1) \log(\alpha_k^m) - b_0^{\alpha}\alpha_k^m + \const
\]
Term from the prior $\log p(\bfw_{:k}^m) = \sum_{d=1}^{D_m} \log p(s_{dk}^m,\hat{w}_{dk}^m)$:
\[
\frac{D_m}{2}\log(\alpha_k^m) -\frac{\alpha_k^m}{2}\sum_{d=1}^{D} \la \hat{w}_{dk}^2 \ra + \sum_{d=1}^{D_m} \{ \la s_{dk}^m \ra \log\theta_0 + (1-\la s_{dk}^m \ra) \log(1-\theta_0)  \} + \const
\]
Writing everything together:
\[
\Big(a_0^{\alpha} + \frac{D_m}{2} - 1 \Big)\log\alpha_k^m - \Big(b_0^{\alpha} + \frac{1}{2} \sum_{d=1}^{D_m}\la (\hat{w}_{d,k}^m)^2 \ra \Big) \alpha_k^m + \const
\]
Variational distribution:
\[
q(\balpha) = \prod_{m=1}^{M} \prod_{k=1}^{K} \mathcal{G}(\alpha_k^m | \hat{a}_{mk}^{\alpha}, \hat{b}_{mk}^{\alpha})
\]
where
\[
\hat{a}_{mk}^\alpha = a_0^\alpha + \frac{D_m}{2}
\]
\[
\hat{b}_{mk}^\alpha = b_0^\alpha +\frac{ \sum_{d=1}^{D_m} \la (\hat{w}_{d,k}^m)^2 \ra }{2}
\]

\subsubsection*{Noise precision (tau)}
Term from the prior $p(\tau_d^m)$:
\[
(a_0^\tau - 1) \log \tau_d^m - b_0^\tau \tau_d^m + \const
\]
Term from the likelihood $\Ndist{y_{n,d}^m}{\sum_{k=1}^{K}\hat{w}_{dk}^m s_{dk}^m z_{nk},\tau_d^m}$:
\[
\frac{N}{2}\log \tau_d^m - \frac{\tau_d^m}{2} \sum_{n=1}^{N} \la (y_{nd}^m - \sum_k^{K} w_{dk}^m s_{dk}^m z_{nk})^2 \ra + \const
\]
Rewriting everything together:
\[
\Big(a_0^\tau - 1 + \frac{N}{2} \Big)\log \tau_d^m - \Big( b_0 + \frac{1}{2} \sum_{n=1}^{N} \la (y_{nd}^m - \sum_k^{K} \hat{w}_{dk}^m s_{dk}^m z_{nk})^2 \ra  \Big )\tau_d^m
\]
Variational distribution:
\[
q(\btau) = \prod_{m=1}^{M} \prod_{d=1}^{D_m} q(\tau_d^m) = \prod_{m=1}^{M} \prod_{d=1}^{D_m} \mathcal{G}(\tau_d^m|\hat{a}_{md}^{\tau},\hat{b}_{md}^{\tau})
\]
where
\[
\hat{a}_{md}^{\tau} = a_0^{\tau} + \frac{N}{2}
\]
\[
\hat{b}_{md}^{\tau} = b_0^{\tau} + \frac{1}{2} \sum_{n=1}^{N} \la (y_{nd}^m - \sum_k^{K} \hat{w}_{dk}^m s_{dk}^m z_{n,k})^2 \ra
\]

\subsubsection*{Spike and Slab sparsity parameter $\theta$}
Unless a given factor is specifically annotated in a given view, the sparisty parameter $\theta^m_{k}$ of the Spike and Slab prior on $w_{k,d}^m, \forall d$ is given a Beta prior: $P(\theta_k^m) = \mathrm{Beta} (a_0, b_0)$. The postrerior $q(\theta_k^m)$ is Beta distributed and the update of its parameters $a_k^m$ and $b_k^m$ are given below:\\

\begin{equation*}
\begin{aligned}
&a_{k}^m = \sum_{d} \la S^m_{k,d}\ra + a_0\\
&b_{k}^m = b_0 - \sum_{d} \la S^m_{k,d}\ra + D_m
\end{aligned}
\end{equation*}

\subsection*{Lower bound}

\subsubsection{Likelihood term}
%Matrix form:
%\[
%\]
Vector form:
% \[
% -\frac{DN}{2} \log(2\pi) + \frac{N}{2}\sum_{d=1}^{D}\log(\tau_d) - \frac{1}{2} \sum_{d=1}^{D} \big( \bfy_d - \sum_{k=1}^{K}\la s_{dk} \hat{w}_{dk} \ra \la \bfz_k \ra \big)^T \big( \tau_d \bfI \big) \big( \bfy_d^T - \sum_{k=1}^{K}\la s_{dk} \hat{w}_{dk} \ra \la \bfz_k \ra \big)
% \]
\[
-\sum_{m=1}^M \frac{ND_m}{2} \log(2\pi) + \frac{N}{2}\sum_{m=1}^{M}\sum_{d=1}^{D_m}\log(\tau_d^m) - \frac{1}{2} \sum_{m=1}^{M}\sum_{d=1}^{D_m} \big( \bfy_d^m - \sum_{k=1}^{K}\la s_{dk}^m \hat{w}_{dk}^m \ra \la \bfz_k \ra \big)^T \big( \tau_d^m \bfI \big) \big( \bfy_d^m - \sum_{k=1}^{K}\la s_{dk}^m \hat{w}_{dk}^m \ra \la \bfz_k \ra \big)
\]

Scalar form:
% \[
% -\frac{DN}{2} \log(2\pi) + \frac{N}{2}\sum_{d=1}^{D}\log(\la \tau_d \ra) - \sum_{d=1}^{D} \frac{\la \tau_d \ra}{2} \sum_{n=1}^{N} \big( y_{nd} - \sum_{k=1}^{K}\la s_{dk} \hat{w}_{dk} \ra \la z_{nk} \ra \big)^2
% \]
\[
-\sum_{m=1}^M \frac{ND_m}{2} \log(2\pi) + \frac{N}{2} \sum_{m=1}^M \sum_{d=1}^{D_m} \log(\la \tau_d^m \ra) -\sum_{m=1}^M \sum_{d=1}^{D_m} \frac{\la \tau_d^m \ra}{2} \sum_{n=1}^{N} \big( y_{nd}^m - \sum_{k=1}^{K}\la s_{dk}^m \hat{w}_{dk}^m \ra \la z_{nk} \ra \big)^2
\]
Extending terms and rearranging:
% \begin{align*}
% &-\frac{DN}{2} \log(2\pi) + \frac{N}{2} \sum_{d=1}^{D}\log(\tau_d) - \frac{1}{2} \sum_{d=1}^{D} \tau_d \bfy_d^T \bfy_d\\
% &+ \sum_{k=1}^{K} \Big( \sum_{d=1}^{D} \tau_d \la s_{dk} \hat{w}_{dk} \ra \bfy_d \Big)^T \la \bfz_k \ra \\
% &- \frac{1}{2} \sum_{k=1}^{K} \Big( \sum_{d=1}^{D} \tau_d \la s_{dk} \hat{w}_{dk}^2 \ra \Big) \la \bfz_k^T \bfz_k \ra\\
% &- \sum_{k=1}^{K} \Big( \sum_{k'=k+1}^{K} \Big( \sum_{d=1}^{D} \tau_d \la s_{dk} \hat{w}_{dk} \ra \la s_{dk'} \hat{w}_{dk'} \ra \Big) \la \bfz_{k'} \ra \Big )^T \la \bfz_k \ra
% \end{align*}

\subsubsection{W and S terms}
$p(\hat{\bfW},\bfS)$:
\begin{align*}
-\sum_{m=1}^{M}\frac{KD_m}{2}\log(2\pi) + \sum_{m=1}^{M}\frac{D_m}{2}\sum_{k=1}^{K} \log(\alpha_k^m) - \sum_{m=1}^{M} \frac{\alpha_k^m}{2} \sum_{d=1}^{D_m} \sum_{k=1}^{K} \la (\hat{w}_{dk}^m)^2 \ra \\
+ \la \log(\theta) \ra \sum_{m=1}^{M} \sum_{d=1}^{D_m} \sum_{k=1}^{K} \la s_{dk}^m \ra + \la \log(1-\theta) \ra \sum_{m=1}^{M} \sum_{d=1}^{D_m}\sum_{k=1}^{K} (1- \la s_{dk}^m \ra)
\end{align*}
%$p(\hat{\bfW})$:
%\[
%-\frac{KD}{2}\log(2\pi) + \frac{D}{2}\sum_{k=1}^{K} \log(\alpha_k) - \frac{\alpha_k}{2} \sum_{d=1}^{D} \sum_{k=1}^{K} \la \hat{w}_{dk}^2 \ra
%\]
$q(\hat{\bfW},\bfS)$:
%\[
%\frac{DK}{2}\log(2\pi) + \frac{DK}{2}\log(\sigma_w^2) - \frac{1}{2} \log(\sigma_w^2) \sum_{d=1}^{D}\sum_{k=1}^{K} \la s_{dk} \ra + \frac{1}{2}  \sum_{d=1}^{D}\sum_{k=1}^{K} \la s_{dk} \ra \log(\sigma_w_{dk}^2)
%\]
\begin{align*}
-\sum_{m=1}^{M}\frac{KD_m}{2}\log(2\pi) + \frac{1}{2}\sum_{m=1}^{M}\sum_{d=1}^{D_m}\sum_{k=1}^{K}\log(\la s_{dk}^m \ra \sigma_{w_{dk}^m}^2 + (1-\la s_{dk}^m \ra)/\alpha_k^m) \\
+ \sum_{m=1}^{M} \sum_{d=1}^{D_m} \sum_{k=1}^{K} (1-\la s_{dk}^m \ra) \log(1 - \la s_{dk}^m \ra) - \la s_{dk}^m \ra \log \la s_{dk}^m \ra
\end{align*}
%$p(\bfS)$:
%\[
%\la \log(\theta) \ra \sum_{d=1}^{D}\sum_{k=1}^{K} \la s_{dk} \ra + \la \log(1-\theta) \ra \sum_{d=1}^{D}\sum_{k=1}^{K} (1- \la s_{dk} \ra)
%\]
%$q(\bfS)$:
%\[
%\sum_{d=1}^{D}\sum_{k=1}^{K} (1-\la s_{dk} \ra) \log(1 - \la s_{dk} \ra) - \la s_{dk} \ra \log \la s_{dk} \ra
%\]

\subsubsection{Z term}
\[
\mathbb{E} [\log P(\bfZ)] = -\frac{NK}{2}\log(2\pi) -\frac{1}{2} \sum_{n=1}^{N} \la z_{nk}^2 \ra
\]
\[
\mathbb{E} [\log q(\bfZ)] = - \frac{NK}{2}(1 + \log(2\pi)) - \frac{1}{2}\sum_{n=1}^{N}\sum_{k=1}^{K} \log(\sigma_{z_{nk}}^2)
\]

%\section{Variational inference}

\subsubsection*{alpha term}
\[
\mathbb{E} [\log p(\balpha)] = \sum_{m=1}^{M}\sum_{k=1}^{K}\Big(a_0^\alpha\log b_0^\alpha +   (a_0^\alpha - 1) \la \log \alpha_k \ra - b_0^\alpha \la \alpha_k \ra - \log \Gamma(a_0^\alpha) \Big)
\]
\[
\mathbb{E} [\log q(\balpha)] = \sum_{m=1}^{M}\sum_{k=1}^{K} \Big( \hat{a}_{k}^\alpha \log \hat{b}_{k}^\alpha + (\hat{a}_{k}^\alpha - 1) \la \log \alpha_k \ra - \hat{b}_{k}^\alpha \la \alpha_k \ra - \log \Gamma(\hat{a}_{k}^\alpha) \Big)
\]

\subsection*{tau}
%\[
%\mathbb{E} [\log P(\btau)] = \sum_{m=1}^{M}\sum_{d=1}^{Dm} a_0^\tau \log b_0^\tau + (a_0^\tau - 1) \la \log %\tau_d^m \ra - b_0^\tau \la \tau_d^m \ra - \log \Gamma(a_0^\tau)
%\]
\[
\mathbb{E} [\log P(\tau)] = \sum_{m=1}^{M} D_m a_0^\tau \log b_0^\tau + \sum_{m=1}^{M}\sum_{d=1}^{Dm} (a_0^\tau - 1) \la \log \tau_d^m \ra - \sum_{m=1}^{M}\sum_{d=1}^{Dm} b_0^\tau \la \tau_d^m \ra - \sum_{m=1}^{M} D_m \Gamma(a_0^\tau)
\]

\[
\mathbb{E} [\log Q(\btau)] = \sum_{m=1}^{M}\sum_{d=1}^{D_m} \Big( \hat{a}_{dm}^\tau \log \hat{b}_{dm}^\tau + (\hat{a}_{dm}^\tau - 1) \la \log \tau_d^m \ra - \hat{b}_{dm}^\tau \la \tau_d^m \ra - \log \Gamma(\hat{a}_{dm}^\tau) \Big)
\]

\subsection*{Theta}
\begin{equation*}
\begin{aligned}
&\mathbb{E}\left[ \log P(\theta) \right] = \sum_{m=1}^M \sum_{k=1}^K\sum_{d=1}^{D_m}\left( (a_0 - 1) \times \la \log(\pi^m_{d, k}) \ra + (b_0 -1) \la \log(1 - \pi^m_{d, k}) \ra - \log (\mathrm{B} (a_0, b_0))\right) \\
&\mathbb{E}\left[ \log Q(\theta) \right] = \sum_{m=1}^M \sum_{k=1}^K\sum_{d=1}^{D_m}\left( (a^m_{k,d} - 1) \times \la \log(\pi^m_{d, k}) \ra + (b^m_{k,d} -1) \la \log(1 - \pi^m_{d, k}) \ra - \log (\mathrm{B} (a^m_{k,d}, b^m_{k,d})) \right) \\
\end{aligned}
\end{equation*}

\subsection{Extensions to non-gaussian likelihoods}

To implement efficient variational inference in conjunction with a non-Gaussian likelihood we adapt prior work from \cite{seeger} using local variational bounds. The key idea is to approximate non-gaussian data by Gaussian pseudo-data based on a second-order Taylor expansion.  To make the approximation justifiable we need to introduce variational parameters that are adjusted alongside the updates to improve the fit.	\\
Denoting the parameters in the MOFA model as $\bTheta= (\bfZ,\bfW,\balpha,\bTau,\btheta)$, the variational framework approximates the posterior $p(\bTheta | \bfY )$ with a distribution $q(\bTheta)$, which is indirectly optimised by optimising a lower bound $\mathcal{F}$ of the log model evidence. The resulting optimization problem is given by
\[
\min_{q(\bTheta)} \mathcal{F} = \min_{q(\bTheta)} \E_q \big[ -\log p(\bfY|\bTheta) \big] + \KL[q(\Theta)||p(\Theta)].
\]
Expanding the MOFA model to non-gaussian likelihoods we now assume a general likelihood of the form $p(\bfY|\bfX)$ with $\bfX = \bfZ\bfW^{T}$, that can write as

\[
-\log p(\bfY|\bTheta) = \sum_{n=1}^{N} \sum_{d=1}^{D} f_{nd} (x_{nd})
\]
with $f_{nd} = -\log p(y_{nd}|x_{nd})$. We dropped the view index $m$ to keep notation uncluttered.\\
Extending \cite{seeger} to our heteroscedastic noise model, we require $f_{nd}(x_{nd})$ to be twice differentiable and bounded by $\kappa_d$, such that $f_{nd}''(x_{nd}) \leq \kappa_d \forall n,d$. This holds true in many important models like for example the Bernoulli and Poisson case. Under this assumption a lower bound on the log likelihood can be constructed using Taylor expansion,
\[
f_{nd}(x_{nd}) \leq \frac{\kappa_d}{2} (x_{nd} + \zeta_{nd})^2 + f'(\zeta_{nd})(x_{nd} - \zeta_{nd}) + f_{nd}(\zeta_{nd}) := q_{nd}(x_{ng},\zeta_{nd}),
\]
where $\bZeta =  \zeta_{nd} $ are additional variational parameters that determine the location of the Taylor expansion and have to be optimised to make the lower bound as tight as possible. Plugging the bounds into above optimization problem, we obtain:
\[
\min_{q(\bTheta),\bZeta} \sum_{d=1}^{D}\sum_{n=1}^{N} \E_q [ q_{nd}(x_{nd}|\zeta_{nd}) + \KL[q(\bTheta)||p(\bTheta)]
\]
The algorithm propsed in \cite{seeger} then alternates between updates of $\bZeta$ and $\mathrm{q}(\bTheta)$. The update for $\bZeta$ is given by
\[
\zeta \leftarrow \E[\bfW]\E[\bfZ]^{T}
\]
where the expctations are taken with respect to the corresponding $q$ distributions.\\
% In order to find the updates for $q(\bTheta)$ we bring the taylor approximation of $q(f_{nd})$ in q audratic form:
% \[
% q(f_{nd},\zeta_{nd}) \propto \frac{\kappa_d}{2}(f_{nd} - (zeta_{nd} - g(\zeta_{nd})/\kappa_d))^2
% \]
% and note that this is proportional to the log of a gaussian distribution $-log \Normal (\hat{y}_{nd}|f_{ng},\frac{1}{ng})$ where $\hat{y}_{nd} = zeta_{nd} - g'(\zeta_{nd})/\kappa_d$ is defined as a pseudodata based on the zero-inflated observations.
% Consequently, for fixed $\zeta_{nd}$, the updates of the variational distributions $Q(X)$ and $Q(W)$ are equivalent to the ones derived in X, but with pseudodata $\hat{Y}$ and precision $\kappa_g$
On the other hand, the updates for $q(\bTheta)$ can be shown to be identical to the variational bayesian updates with a conjugated gaussian likelihood when replacing the observed data $\bfY$ by a pseudo-data $\hat{\bfY}$ and the precisions $\tau_{nd}$ (which were treated as random variables) by the constant terms $\kappa_d$ introduced above.\\
The pseudodata is given by
\[
\hat{y_{nd}} = \zeta_{nd} - f'(\zeta_{nd})/\kappa_d.
\]
Depending on the log likelihoods $f(·)$ different $\kappa_d$ are used resulting in different pseudo-data updates. Two special cases implemented in MOFA are the Poisson and the Bernoulli likelihood:

\subsubsection{Bernoulli likelihood for binary data}
When the observations are binary, $y \in \{0,1\}$, they can be modelled using a Bernoulli likelihood:
\[
p(y|x) = \frac{e^{yx}}{1+e^x}
\]
The second derivative of the log likelihood is bounded by:
\[
f''(x) = \sigma(x)\sigma(-x) \leq 1/4 := \kappa
\]
where $\sigma$ is the sigmoid function $f(x) = 1/(1+e^{-x})$.\\
The pseudodata updates are given by
\begin{equation*}
\hat{y}_{nd} = \zeta_{nd} - 4*(\sigma(\zeta_{nd}) - y_{nd})
\end{equation*}


\subsubsection*{A tighter bound for binary data}
While the above approach of efficient variational inference for non-gaussian likelihoods works well in many settings, it is possible to further improve the approximation when using heteroscedatic gaussian pseudo-data instead of a spherical gaussian. MOFA uses this to implement a tighter bound for binary data, which frequently occurs as a view in multi-omic data sets (e.g. somatic mutations or SNPs).
As before we model binary data $Y$ as 
\begin{equation*}
\bfY|\bfZ,\bfW \sim \text{Ber}(\sigma(\bfZ\bfW^T)),
\end{equation*} where $\sigma(a)=(1+e^{-a})^{-1}$ is the logistic link function and $\bfZ$ and $\bfW$ are the latent factors and weights in our model, respectively.\\
In order to make the variational  inference efficient and explicit as in the Gaussian case, we aim to approximate the Bernoulli data by a Gaussian pseudo-data as proposed in \cite{seeger} and described above which allows to recycle all the updates from the model with Gaussian views. While \cite{seeger} assumes a homoscedastic approximation with a spherical Gaussian, we adopt an approach following \cite{Jaakkola}, which allows for heteroscedaticity and provides a tighter bound on the Bernoulli likelihood.\\
Denoting $x_{ij}=(\bfZ\bfW^T)_{ij}$ the Jaakkola upper bound \cite{Jaakkola} on the negative log-likelihood is given by
\begin{align}
\begin{split}
-\log\left(p(y_{ij}|x_{ij})\right) &= -\log\left(\sigma\left((2y_{ij}-1)  x_{ij}\right)\right)\\
& \leq -\log(\zeta_{ij})-\frac{(2y_{ij}-1)x_{ij}-\zeta_{ij})}{2} +\lambda(\zeta_{ij})\left(x_{ij}^2 -\zeta_{ij}^2 \right)\\
& =: b_J(\zeta_{ij}, x_{ij},y_{ij} )
\label{jaakkola}
\end{split}
\end{align}
with $\lambda$ given by $\lambda(\zeta)=\frac{1}{4\zeta}\tanh\left(\frac{\zeta}{2}\right)$.\\
This can be derived easily from a first-order Taylor expansion on the function $f(x) = - \log(e^{\frac{x}{2}}+e^{-\frac{x}{2}}) = \frac{x}{2}-\log(\sigma(x))$ in $x^2$ and by the convexity of 
$f$ in $x^2$ this bound is global as discussed in \cite{Jaakkola}.\\
In order to make use of this tighter bound but still be able to re-use the variational updates from the Gaussian case we re-formulate the bound as a Gaussian likelihood on pseudo-data $\tilde{Y}$.\\
As above we can plug in the bound on the negative log-likelihood in the variational optimization problem to obtain  \[
\min_{q(\bTheta),\bZeta} \sum_{d=1}^{D}\sum_{n=1}^{N} \mathbb{E}_q b_J(\zeta_{ij}, x_{ij},y_{ij} ) + \KL[q(\bTheta)||p(\bTheta)].
\]
This is minimized iteratively in the variational parameter $\zeta_{ij}$ and the variational distribution of Z,W:\\
Minimizing in the variational parameter $\zeta$ this leads to the updates given by
\begin{equation*}
\zeta_{ij}^2 = \mathbb{E}[x_{ij}^2]
\end{equation*}
as described in \cite{Jaakkola}, \cite{bishop2006pattern}.\\
For the variational distribution $q(\bfZ,\bfW)$ we observe that the Jaakkola bound can be re-written as 
\begin{equation*}
b_J(\zeta_{ij}, x_{ij},y_{ij} ) = -\log\left(\varphi\left(\tilde{y}_{ij}; x_{ij}, \frac{1}{2\lambda(\zeta_{ij})}\right)\right) + c(\zeta_{ij}),
\end{equation*}
where $\varphi(x; \mu, \sigma^2)$ denotes the density function of a normal distribution with mean $\mu$ and variance $\sigma^2$ and c is a term only depending on $\bZeta$. This allows us to re-use the updates for $\bfZ$ and $\bfW$ from a setting with Gaussian likelihood by considering the Gaussian pseudo-data 
\begin{equation*}
\tilde{y}_{ij}= \frac{2y_{ij}-1}{4 \lambda(\zeta_{ij})}
\end{equation*}
updating the data precision as $\tau_{ij} = 2\lambda(\zeta_{ij})$ using updates that allow for sample- and feature-wise precision parameters on the data as described in the \nameref{appendix}.


\subsubsection{Poisson likelihood for count data}
When observations are a natural numbers, such as count data $y \in \N = \{0,1,\cdots\}$, they can be modelled using a Poisson likelihood:
\[
p(y|x) = \lambda(x)^y e^{-\lambda(x)}
\]
where $\lambda(x)>0$ is the rate function and has to be convex and log-concave in order to ensure that the likelihood is log-concave.\\
As done in \cite{seeger}, here we choose the following rate function: $\lambda(x)=\log(1+e^x)$.

Then an upper bound of the second derivative of the log-likelihood is given by
\[
f''_{nd}(x_{nd}) \leq \kappa_d = 1/4 + 0.17*\max(\bfy_{:,d})
\]
%The bound degrades with the presence of entries with large values. Thus, we follow common practice and clip overly large counts.\\
The pseudodata updates are given by
\[
\hat{y}_{nd} = \zeta_{nd} - \frac{\mathrm{S}(\zeta_{nd})(1-y_{nd}/\lambda(\zeta_{nd}))}{\kappa_d}
\]




\printbibliography

\end{document}
